# 🎤 Voice Classification using MFCC + Spectral Contrast Features

This project focuses on **classifying audio samples into multiple voice classes** using extracted MFCC and Spectral Contrast features.  
It implements a clean and reproducible **PyTorch pipeline** that covers feature extraction, model training, evaluation, and submission generation.

---

## 🗂️ Folder Structure

```
project/
├── train/                     # Training audio files (hosted on Google Drive)
├── test/                      # Test audio files (hosted on Google Drive)
├── metadata.csv               # Contains mapping of Label IDs to their class names
├── train.csv                  # Contains file names and target labels for training
├── test.csv                   # Contains file names for test samples
├── submission.csv             # Final prediction output generated by the model
├── main.py                    # Training + inference script (creates saved model)
├── evaluate_taskA.py          # Evaluation-only script (loads saved model)
└── saved_model/
    └── voice_classifier.pth   # Trained PyTorch model checkpoint
```

---

## 📦 Dataset Access

place the dataset folders as shown above inside your project directory.
as shown in folder structure

---

## ⚙️ Requirements

Install dependencies before running:
```bash
pip install torch torchvision torchaudio librosa pandas numpy scikit-learn tqdm
```

---

## 🚀 How to Run

### 1️⃣ Train the Model
Run the following command to start training and automatically generate predictions:
```bash
python main.py
```

This will:
- Load dataset metadata (`train.csv`, `metadata.csv`)
- Extract audio features (MFCC + Spectral Contrast)
- Train the neural network
- Save the model in `saved_model/voice_classifier.pth`
- Generate `submission.csv` for test data

---

### 2️⃣ Evaluate Using Saved Model
Once training is complete, you can evaluate the trained model without retraining:
```bash
python evaluate_taskA.py
```

This will:
- Load the pre-trained model from `saved_model/voice_classifier.pth`
- Evaluate on validation/test data
- Generate an updated `submission.csv` file with predictions

---

## 🧠 Model Architecture

The model is a **fully connected feedforward neural network** designed for compact feature representations.

```
Input (MFCC + Spectral Contrast) → Linear(128) → ReLU → Dropout(0.3)
→ Linear(64) → ReLU → Linear(num_classes)
```

Loss Function: **CrossEntropyLoss**  
Optimizer: **Adam** (lr = 0.001)

---

## 🎧 Feature Extraction

Each audio file is processed as follows:
1. Loaded using `librosa` at 16 kHz sampling rate.  
2. Extracts **20 MFCCs** and **7 Spectral Contrast features**.  
3. Mean-pooled and concatenated into a single feature vector of **27 dimensions** per file.

---

## 📊 Evaluation Metrics

During evaluation, the model reports:
- **Accuracy**
- **Precision**
- **Recall**
- **F1-Score (Macro and Weighted)**

Result after training using given dataset:
```
✅ Accuracy: 97.02%
🎯 Macro F1: 96.09%
```

---

## 🏁 Output

The final predictions are stored in:
```
submission.csv
```

Format:
| File_name | target |
|------------|---------|
| audio_001.wav | 3 |
| audio_002.wav | 1 |
| audio_003.wav | 2 |

---

---

## 📘 License

This project is released under the **MIT License** — free for educational and research use.
