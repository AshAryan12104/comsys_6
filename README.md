# ğŸ¤ Voice Classification using MFCC + Spectral Contrast Features

This project focuses on **classifying audio samples into multiple voice classes** using extracted MFCC and Spectral Contrast features.  
It implements a clean and reproducible **PyTorch pipeline** that covers feature extraction, model training, evaluation, and submission generation.

---

## ğŸ—‚ï¸ Folder Structure

```
project/
â”œâ”€â”€ train/                     # Training audio files (hosted on Google Drive)
â”œâ”€â”€ test/                      # Test audio files (hosted on Google Drive)
â”œâ”€â”€ metadata.csv               # Contains mapping of Label IDs to their class names
â”œâ”€â”€ train.csv                  # Contains file names and target labels for training
â”œâ”€â”€ test.csv                   # Contains file names for test samples
â”œâ”€â”€ submission.csv             # Final prediction output generated by the model
â”œâ”€â”€ main.py                    # Training + inference script (creates saved model)
â”œâ”€â”€ evaluate_taskA.py          # Evaluation-only script (loads saved model)
â””â”€â”€ saved_model/
    â””â”€â”€ voice_classifier.pth   # Trained PyTorch model checkpoint
```

---

## ğŸ“¦ Dataset Access

place the dataset folders as shown above inside your project directory.
as shown in folder structure

---

## âš™ï¸ Requirements

Install dependencies before running:
```bash
pip install torch torchvision torchaudio librosa pandas numpy scikit-learn tqdm
```

---

## ğŸš€ How to Run

### 1ï¸âƒ£ Train the Model
Run the following command to start training and automatically generate predictions:
```bash
python main.py
```

This will:
- Load dataset metadata (`train.csv`, `metadata.csv`)
- Extract audio features (MFCC + Spectral Contrast)
- Train the neural network
- Save the model in `saved_model/voice_classifier.pth`
- Generate `submission.csv` for test data

---

### 2ï¸âƒ£ Evaluate Using Saved Model
Once training is complete, you can evaluate the trained model without retraining:
```bash
python evaluate_taskA.py
```

This will:
- Load the pre-trained model from `saved_model/voice_classifier.pth`
- Evaluate on validation/test data
- Generate an updated `submission.csv` file with predictions

---

## ğŸ§  Model Architecture

The model is a **fully connected feedforward neural network** designed for compact feature representations.

```
Input (MFCC + Spectral Contrast) â†’ Linear(128) â†’ ReLU â†’ Dropout(0.3)
â†’ Linear(64) â†’ ReLU â†’ Linear(num_classes)
```

Loss Function: **CrossEntropyLoss**  
Optimizer: **Adam** (lr = 0.001)

---

## ğŸ§ Feature Extraction

Each audio file is processed as follows:
1. Loaded using `librosa` at 16 kHz sampling rate.  
2. Extracts **20 MFCCs** and **7 Spectral Contrast features**.  
3. Mean-pooled and concatenated into a single feature vector of **27 dimensions** per file.

---

## ğŸ“Š Evaluation Metrics

During evaluation, the model reports:
- **Accuracy**
- **Precision**
- **Recall**
- **F1-Score (Macro and Weighted)**

Result after training using given dataset:
```
âœ… Accuracy: 97.02%
ğŸ¯ Macro F1: 96.09%
```

---

## ğŸ Output

The final predictions are stored in:
```
submission.csv
```

Format:
| File_name | target |
|------------|---------|
| audio_001.wav | 3 |
| audio_002.wav | 1 |
| audio_003.wav | 2 |

---

---

## ğŸ“˜ License

This project is released under the **MIT License** â€” free for educational and research use.
